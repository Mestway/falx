# The file was automatically generated by Lark v0.6.5
#
#
#   Lark Stand-alone Generator Tool
# ----------------------------------
# Generates a stand-alone LALR(1) parser with a standard lexer
#
# Git:    https://github.com/erezsh/lark
# Author: Erez Shinan (erezshin@gmail.com)
#
#
#    >>> LICENSE
#
#    This tool and its generated code use a separate license from Lark.
#
#    It is licensed under GPLv2 or above.
#
#    If you wish to purchase a commercial license for this tool and its
#    generated code, contact me via email.
#
#    If GPL is incompatible with your free or open-source project,
#    contact me and we'll work it out (for free).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 2 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    See <http://www.gnu.org/licenses/>.
#
#

class LarkError(Exception):
    pass

class GrammarError(LarkError):
    pass

class ParseError(LarkError):
    pass

class LexError(LarkError):
    pass

class UnexpectedInput(LarkError):
    pos_in_stream = None

    def get_context(self, text, span=40):
        pos = self.pos_in_stream
        start = max(pos - span, 0)
        end = pos + span
        before = text[start:pos].rsplit('\n', 1)[-1]
        after = text[pos:end].split('\n', 1)[0]
        return before + after + '\n' + ' ' * len(before) + '^\n'

    def match_examples(self, parse_fn, examples):
        """ Given a parser instance and a dictionary mapping some label with
            some malformed syntax examples, it'll return the label for the
            example that bests matches the current error.
        """
        assert self.state is not None, "Not supported for this exception"

        candidate = None
        for label, example in examples.items():
            assert not isinstance(example, STRING_TYPE)

            for malformed in example:
                try:
                    parse_fn(malformed)
                except UnexpectedInput as ut:
                    if ut.state == self.state:
                        try:
                            if ut.token == self.token:  # Try exact match first
                                return label
                        except AttributeError:
                            pass
                        if not candidate:
                            candidate = label

        return candidate


class UnexpectedCharacters(LexError, UnexpectedInput):
    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None):
        message = "No terminal defined for '%s' at line %d col %d" % (seq[lex_pos], line, column)

        self.line = line
        self.column = column
        self.allowed = allowed
        self.considered_tokens = considered_tokens
        self.pos_in_stream = lex_pos
        self.state = state

        message += '\n\n' + self.get_context(seq)
        if allowed:
            message += '\nExpecting: %s\n' % allowed

        super(UnexpectedCharacters, self).__init__(message)



class UnexpectedToken(ParseError, UnexpectedInput):
    def __init__(self, token, expected, considered_rules=None, state=None):
        self.token = token
        self.expected = expected     # XXX str shouldn't necessary
        self.line = getattr(token, 'line', '?')
        self.column = getattr(token, 'column', '?')
        self.considered_rules = considered_rules
        self.state = state
        self.pos_in_stream = getattr(token, 'pos_in_stream', None)

        message = ("Unexpected token %r at line %s, column %s.\n"
                   "Expected one of: \n\t* %s\n"
                   % (token, self.line, self.column, '\n\t* '.join(self.expected)))

        super(UnexpectedToken, self).__init__(message)


try:
    STRING_TYPE = basestring
except NameError:   # Python 3
    STRING_TYPE = str


import types
from functools import wraps, partial
from contextlib import contextmanager

Str = type(u'')

def smart_decorator(f, create_decorator):
    if isinstance(f, types.FunctionType):
        return wraps(f)(create_decorator(f, True))

    elif isinstance(f, (type, types.BuiltinFunctionType)):
        return wraps(f)(create_decorator(f, False))

    elif isinstance(f, types.MethodType):
        return wraps(f)(create_decorator(f.__func__, True))

    elif isinstance(f, partial):
        # wraps does not work for partials in 2.7: https://bugs.python.org/issue3445
        return create_decorator(f.__func__, True)

    else:
        return create_decorator(f.__func__.__call__, True)



class Meta:
    pass

class Tree(object):
    def __init__(self, data, children, meta=None):
        self.data = data
        self.children = children
        self._meta = meta

    @property
    def meta(self):
        if self._meta is None:
            self._meta = Meta()
        return self._meta

    def __repr__(self):
        return 'Tree(%s, %s)' % (self.data, self.children)

    def _pretty_label(self):
        return self.data

    def _pretty(self, level, indent_str):
        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
            return [ indent_str*level, self._pretty_label(), '\t', '%s' % (self.children[0],), '\n']

        l = [ indent_str*level, self._pretty_label(), '\n' ]
        for n in self.children:
            if isinstance(n, Tree):
                l += n._pretty(level+1, indent_str)
            else:
                l += [ indent_str*(level+1), '%s' % (n,), '\n' ]

        return l

    def pretty(self, indent_str='  '):
        return ''.join(self._pretty(0, indent_str))
    def __eq__(self, other):
        try:
            return self.data == other.data and self.children == other.children
        except AttributeError:
            return False

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash((self.data, tuple(self.children)))

from inspect import getmembers, getmro

class Discard(Exception):
    pass

# Transformers

class Transformer:
    """Visits the tree recursively, starting with the leaves and finally the root (bottom-up)

    Calls its methods (provided by user via inheritance) according to tree.data
    The returned value replaces the old one in the structure.

    Can be used to implement map or reduce.
    """

    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            if getattr(f, 'meta', False):
                return f(children, tree.meta)
            elif getattr(f, 'inline', False):
                return f(*children)
            elif getattr(f, 'whole_tree', False):
                if new_children is not None:
                    raise NotImplementedError("Doesn't work with the base Transformer class")
                return f(tree)
            else:
                return f(children)

    def _transform_children(self, children):
        for c in children:
            try:
                yield self._transform_tree(c) if isinstance(c, Tree) else c
            except Discard:
                pass

    def _transform_tree(self, tree):
        children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree, children)

    def transform(self, tree):
        return self._transform_tree(tree)

    def __mul__(self, other):
        return TransformerChain(self, other)

    def __default__(self, data, children, meta):
        "Default operation on tree (for override)"
        return Tree(data, children, meta)

    @classmethod
    def _apply_decorator(cls, decorator, **kwargs):
        mro = getmro(cls)
        assert mro[0] is cls
        libmembers = {name for _cls in mro[1:] for name, _ in getmembers(_cls)}
        for name, value in getmembers(cls):
            if name.startswith('_') or name in libmembers:
                continue

            if isinstance(cls.__dict__[name], (staticmethod, classmethod)):
                kwargs['static'] = True
            setattr(cls, name, decorator(value, **kwargs))
        return cls


class InlineTransformer(Transformer):   # XXX Deprecated
    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = getattr(self, tree.data)
        except AttributeError:
            return self.__default__(tree.data, children, tree.meta)
        else:
            return f(*children)


class TransformerChain(object):
    def __init__(self, *transformers):
        self.transformers = transformers

    def transform(self, tree):
        for t in self.transformers:
            tree = t.transform(tree)
        return tree

    def __mul__(self, other):
        return TransformerChain(*self.transformers + (other,))


class Transformer_InPlace(Transformer):
    "Non-recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):           # Cancel recursion
        return self._call_userfunc(tree)

    def transform(self, tree):
        for subtree in tree.iter_subtrees():
            subtree.children = list(self._transform_children(subtree.children))

        return self._transform_tree(tree)


class Transformer_InPlaceRecursive(Transformer):
    "Recursive. Changes the tree in-place instead of returning new instances"
    def _transform_tree(self, tree):
        tree.children = list(self._transform_children(tree.children))
        return self._call_userfunc(tree)



# Visitors

class VisitorBase:
    def _call_userfunc(self, tree):
        return getattr(self, tree.data, self.__default__)(tree)

    def __default__(self, tree):
        "Default operation on tree (for override)"
        return tree


class Visitor(VisitorBase):
    """Bottom-up visitor, non-recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """


    def visit(self, tree):
        for subtree in tree.iter_subtrees():
            self._call_userfunc(subtree)
        return tree

class Visitor_Recursive(VisitorBase):
    """Bottom-up visitor, recursive

    Visits the tree, starting with the leaves and finally the root (bottom-up)
    Calls its methods (provided by user via inheritance) according to tree.data
    """

    def visit(self, tree):
        for child in tree.children:
            if isinstance(child, Tree):
                self.visit(child)

        f = getattr(self, tree.data, self.__default__)
        f(tree)
        return tree



def visit_children_decor(func):
    "See Interpreter"
    @wraps(func)
    def inner(cls, tree):
        values = cls.visit_children(tree)
        return func(cls, values)
    return inner


class Interpreter:
    """Top-down visitor, recursive

    Visits the tree, starting with the root and finally the leaves (top-down)
    Calls its methods (provided by user via inheritance) according to tree.data

    Unlike Transformer and Visitor, the Interpreter doesn't automatically visit its sub-branches.
    The user has to explicitly call visit_children, or use the @visit_children_decor
    """
    def visit(self, tree):
        return getattr(self, tree.data)(tree)

    def visit_children(self, tree):
        return [self.visit(child) if isinstance(child, Tree) else child
                for child in tree.children]

    def __getattr__(self, name):
        return self.__default__

    def __default__(self, tree):
        return self.visit_children(tree)




# Decorators

def _apply_decorator(obj, decorator, **kwargs):
    try:
        _apply = obj._apply_decorator
    except AttributeError:
        return decorator(obj, **kwargs)
    else:
        return _apply(decorator, **kwargs)



def _inline_args__func(func):
    @wraps(func)
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, children):
                return _f(self, *children)
        else:
            def f(self, children):
                return _f(*children)
        return f

    return smart_decorator(func, create_decorator)


def inline_args(obj):   # XXX Deprecated
    return _apply_decorator(obj, _inline_args__func)



def _visitor_args_func_dec(func, inline=False, meta=False, whole_tree=False, static=False):
    assert [whole_tree, meta, inline].count(True) <= 1
    def create_decorator(_f, with_self):
        if with_self:
            def f(self, *args, **kwargs):
                return _f(self, *args, **kwargs)
        else:
            def f(self, *args, **kwargs):
                return _f(*args, **kwargs)
        return f

    if static:
        f = wraps(func)(create_decorator(func, False))
    else:
        f = smart_decorator(func, create_decorator)
    f.inline = inline
    f.meta = meta
    f.whole_tree = whole_tree
    return f

def v_args(inline=False, meta=False, tree=False):
    "A convenience decorator factory, for modifying the behavior of user-supplied visitor methods"
    if [tree, meta, inline].count(True) > 1:
        raise ValueError("Visitor functions can either accept tree, or meta, or be inlined. These cannot be combined.")
    def _visitor_args_dec(obj):
        return _apply_decorator(obj, _visitor_args_func_dec, inline=inline, meta=meta, whole_tree=tree)
    return _visitor_args_dec



class Indenter:
    def __init__(self):
        self.paren_level = 0
        self.indent_level = [0]

    def handle_NL(self, token):
        if self.paren_level > 0:
            return

        yield token

        indent_str = token.rsplit('\n', 1)[1] # Tabs and spaces
        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len

        if indent > self.indent_level[-1]:
            self.indent_level.append(indent)
            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
        else:
            while indent < self.indent_level[-1]:
                self.indent_level.pop()
                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)

            assert indent == self.indent_level[-1], '%s != %s' % (indent, self.indent_level[-1])

    def process(self, stream):
        for token in stream:
            if token.type == self.NL_type:
                for t in self.handle_NL(token):
                    yield t
            else:
                yield token

            if token.type in self.OPEN_PAREN_types:
                self.paren_level += 1
            elif token.type in self.CLOSE_PAREN_types:
                self.paren_level -= 1
                assert self.paren_level >= 0

        while len(self.indent_level) > 1:
            self.indent_level.pop()
            yield Token(self.DEDENT_type, '')

        assert self.indent_level == [0], self.indent_level

    # XXX Hack for ContextualLexer. Maybe there's a more elegant solution?
    @property
    def always_accept(self):
        return (self.NL_type,)


class Token(Str):
    __slots__ = ('type', 'pos_in_stream', 'value', 'line', 'column', 'end_line', 'end_column')

    def __new__(cls, type_, value, pos_in_stream=None, line=None, column=None):
        self = super(Token, cls).__new__(cls, value)
        self.type = type_
        self.pos_in_stream = pos_in_stream
        self.value = value
        self.line = line
        self.column = column
        self.end_line = None
        self.end_column = None
        return self

    @classmethod
    def new_borrow_pos(cls, type_, value, borrow_t):
        return cls(type_, value, borrow_t.pos_in_stream, line=borrow_t.line, column=borrow_t.column)

    def __reduce__(self):
        return (self.__class__, (self.type, self.value, self.pos_in_stream, self.line, self.column, ))

    def __repr__(self):
        return 'Token(%s, %r)' % (self.type, self.value)

    def __deepcopy__(self, memo):
        return Token(self.type, self.value, self.pos_in_stream, self.line, self.column)

    def __eq__(self, other):
        if isinstance(other, Token) and self.type != other.type:
            return False

        return Str.__eq__(self, other)

    __hash__ = Str.__hash__


class LineCounter:
    def __init__(self):
        self.newline_char = '\n'
        self.char_pos = 0
        self.line = 1
        self.column = 1
        self.line_start_pos = 0

    def feed(self, token, test_newline=True):
        """Consume a token and calculate the new line & column.

        As an optional optimization, set test_newline=False is token doesn't contain a newline.
        """
        if test_newline:
            newlines = token.count(self.newline_char)
            if newlines:
                self.line += newlines
                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1

        self.char_pos += len(token)
        self.column = self.char_pos - self.line_start_pos + 1

class _Lex:
    "Built to serve both Lexer and ContextualLexer"
    def __init__(self, lexer, state=None):
        self.lexer = lexer
        self.state = state

    def lex(self, stream, newline_types, ignore_types):
        newline_types = frozenset(newline_types)
        ignore_types = frozenset(ignore_types)
        line_ctr = LineCounter()

        while line_ctr.char_pos < len(stream):
            lexer = self.lexer
            for mre, type_from_index in lexer.mres:
                m = mre.match(stream, line_ctr.char_pos)
                if not m:
                    continue

                t = None
                value = m.group(0)
                type_ = type_from_index[m.lastindex]
                if type_ not in ignore_types:
                    t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                    if t.type in lexer.callback:
                        t = lexer.callback[t.type](t)
                    yield t
                else:
                    if type_ in lexer.callback:
                        t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
                        lexer.callback[type_](t)

                line_ctr.feed(value, type_ in newline_types)
                if t:
                    t.end_line = line_ctr.line
                    t.end_column = line_ctr.column

                break
            else:
                raise UnexpectedCharacters(stream, line_ctr.char_pos, line_ctr.line, line_ctr.column, state=self.state)


class UnlessCallback:
    def __init__(self, mres):
        self.mres = mres

    def __call__(self, t):
        for mre, type_from_index in self.mres:
            m = mre.match(t.value)
            if m:
                t.type = type_from_index[m.lastindex]
                break
        return t


from functools import partial, wraps


class ExpandSingleChild:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        if len(children) == 1:
            return children[0]
        else:
            return self.node_builder(children)


class PropagatePositions:
    def __init__(self, node_builder):
        self.node_builder = node_builder

    def __call__(self, children):
        res = self.node_builder(children)

        if isinstance(res, Tree):
            res.meta.empty = True

            for c in children:
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.line = c.meta.line
                    res.meta.column = c.meta.column
                    res.meta.start_pos = c.meta.start_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.line = c.line
                    res.meta.column = c.column
                    res.meta.start_pos = c.pos_in_stream
                    res.meta.empty = False
                    break

            for c in reversed(children):
                if isinstance(c, Tree) and c.children and not c.meta.empty:
                    res.meta.end_line = c.meta.end_line
                    res.meta.end_column = c.meta.end_column
                    res.meta.end_pos = c.meta.end_pos
                    res.meta.empty = False
                    break
                elif isinstance(c, Token):
                    res.meta.end_line = c.end_line
                    res.meta.end_column = c.end_column
                    res.meta.end_pos = c.pos_in_stream + len(c.value)
                    res.meta.empty = False
                    break

        return res


class ChildFilter:
    def __init__(self, to_include, node_builder):
        self.node_builder = node_builder
        self.to_include = to_include

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                filtered += children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

class ChildFilterLALR(ChildFilter):
    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"

    def __call__(self, children):
        filtered = []
        for i, to_expand in self.to_include:
            if to_expand:
                if filtered:
                    filtered += children[i].children
                else:   # Optimize for left-recursion
                    filtered = children[i].children
            else:
                filtered.append(children[i])

        return self.node_builder(filtered)

def _should_expand(sym):
    return not sym.is_term and sym.name.startswith('_')

def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous):
    to_include = [(i, _should_expand(sym)) for i, sym in enumerate(expansion)
                  if keep_all_tokens or not (sym.is_term and sym.filter_out)]

    if len(to_include) < len(expansion) or any(to_expand for i, to_expand in to_include):
        return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include)


class Callback(object):
    pass


def ptb_inline_args(func):
    @wraps(func)
    def f(children):
        return func(*children)
    return f



class ParseTreeBuilder:
    def __init__(self, rules, tree_class, propagate_positions=False, keep_all_tokens=False, ambiguous=False):
        self.tree_class = tree_class
        self.propagate_positions = propagate_positions
        self.always_keep_all_tokens = keep_all_tokens
        self.ambiguous = ambiguous

        self.rule_builders = list(self._init_builders(rules))

        self.user_aliases = {}

    def _init_builders(self, rules):
        for rule in rules:
            options = rule.options
            keep_all_tokens = self.always_keep_all_tokens or (options.keep_all_tokens if options else False)
            expand_single_child = options.expand1 if options else False

            wrapper_chain = filter(None, [
                (expand_single_child and not rule.alias) and ExpandSingleChild,
                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous),
                self.propagate_positions and PropagatePositions,
            ])

            yield rule, wrapper_chain


    def create_callback(self, transformer=None):
        callback = Callback()

        i = 0
        for rule, wrapper_chain in self.rule_builders:
            internal_callback_name = '_cb%d_%s' % (i, rule.origin)
            i += 1

            user_callback_name = rule.alias or rule.origin.name
            try:
                f = getattr(transformer, user_callback_name)
                assert not getattr(f, 'meta', False), "Meta args not supported for internal transformer"
                # XXX InlineTransformer is deprecated!
                if getattr(f, 'inline', False) or isinstance(transformer, InlineTransformer):
                    f = ptb_inline_args(f)
            except AttributeError:
                f = partial(self.tree_class, user_callback_name)

            self.user_aliases[rule] = rule.alias
            rule.alias = internal_callback_name

            for w in wrapper_chain:
                f = w(f)

            if hasattr(callback, internal_callback_name):
                raise GrammarError("Rule '%s' already exists" % (rule,))
            setattr(callback, internal_callback_name, f)

        return callback



class _Parser:
    def __init__(self, parse_table, callbacks):
        self.states = parse_table.states
        self.start_state = parse_table.start_state
        self.end_state = parse_table.end_state
        self.callbacks = callbacks

    def parse(self, seq, set_state=None):
        token = None
        stream = iter(seq)
        states = self.states

        state_stack = [self.start_state]
        value_stack = []

        if set_state: set_state(self.start_state)

        def get_action(token):
            state = state_stack[-1]
            try:
                return states[state][token.type]
            except KeyError:
                expected = [s for s in states[state].keys() if s.isupper()]
                raise UnexpectedToken(token, expected, state=state)

        def reduce(rule):
            size = len(rule.expansion)
            if size:
                s = value_stack[-size:]
                del state_stack[-size:]
                del value_stack[-size:]
            else:
                s = []

            value = self.callbacks[rule](s)

            _action, new_state = states[state_stack[-1]][rule.origin.name]
            assert _action is Shift
            state_stack.append(new_state)
            value_stack.append(value)

        # Main LALR-parser loop
        for token in stream:
            while True:
                action, arg = get_action(token)
                assert arg != self.end_state

                if action is Shift:
                    state_stack.append(arg)
                    value_stack.append(token)
                    if set_state: set_state(arg)
                    break # next token
                else:
                    reduce(arg)

        token = Token.new_borrow_pos('$END', '', token) if token else Token('$END', '', 0, 1, 1)
        while True:
            _action, arg = get_action(token)
            if _action is Shift:
                assert arg == self.end_state
                val ,= value_stack
                return val
            else:
                reduce(arg)


class Symbol(object):
    is_term = NotImplemented

    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        assert isinstance(other, Symbol), other
        return self.is_term == other.is_term and self.name == other.name

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        return hash(self.name)

    def __repr__(self):
        return '%s(%r)' % (type(self).__name__, self.name)

    fullrepr = property(__repr__)

class Terminal(Symbol):
    is_term = True

    def __init__(self, name, filter_out=False):
        self.name = name
        self.filter_out = filter_out

    @property
    def fullrepr(self):
        return '%s(%r, %r)' % (type(self).__name__, self.name, self.filter_out)


class NonTerminal(Symbol):
    is_term = False

class Rule(object):
    """
        origin : a symbol
        expansion : a list of symbols
    """
    def __init__(self, origin, expansion, alias=None, options=None):
        self.origin = origin
        self.expansion = expansion
        self.alias = alias
        self.options = options

    def __str__(self):
        return '<%s : %s>' % (self.origin, ' '.join(map(str,self.expansion)))

    def __repr__(self):
        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)


class RuleOptions:
    def __init__(self, keep_all_tokens=False, expand1=False, priority=None):
        self.keep_all_tokens = keep_all_tokens
        self.expand1 = expand1
        self.priority = priority

    def __repr__(self):
        return 'RuleOptions(%r, %r, %r)' % (
            self.keep_all_tokens,
            self.expand1,
            self.priority,
        )

Shift = 0
Reduce = 1
import re
class LexerRegexps: pass
NEWLINE_TYPES = ['COMMENT', 'STRLIT', 'WS']
IGNORE_TYPES = ['WS', 'COMMENT']
LEXERS = {}
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[0] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[1] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[2] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[3] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[4] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[5] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[6] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[7] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PROGRAM>program)',
  {1: 'COMMENT', 3: 'WS', 4: 'PROGRAM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[8] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[9] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[10] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LSQB>\\[)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'LSQB',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[11] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[12] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LSQB>\\[)',
  {1: 'COMMENT', 3: 'WS', 4: 'LSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[13] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[14] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[15] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[16] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[17] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[18] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[19] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[20] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[21] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)',
  {1: 'COMMENT', 3: 'WS', 4: 'INT'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[22] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[23] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[24] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[25] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON', 5: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[26] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[27] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[28] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[29] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[30] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[31] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[32] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[33] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[34] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RSQB>\\])',
  {1: 'COMMENT', 3: 'WS', 4: 'RSQB'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[35] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[36] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[37] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[38] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[39] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[40] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[41] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[42] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[43] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[44] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[45] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<BOOL>bool)|(?P<__ANON_8>int)',
  {1: 'COMMENT', 3: 'WS', 4: 'BOOL', 5: '__ANON_8'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[46] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[47] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[48] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[49] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[50] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[51] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[52] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[53] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[54] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[55] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[56] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[57] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[58] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n|\\(\\*(.|\n)+\\*\\)))|(?P<WS>(?:[ \t\x0c\r\n])+)',
  {1: 'COMMENT', 3: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[59] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[60] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COLON>:)',
  {1: 'COMMENT', 3: 'WS', 4: 'COLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[61] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[62] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[63] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[64] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[65] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[66] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ENUMSET>enumset)|(?P<PROGRAM>program)|(?P<VALUE>value)|(?P<ENUM>enum)',
  {1: 'COMMENT', 3: 'WS', 4: 'ENUMSET', 5: 'PROGRAM', 6: 'VALUE', 7: 'ENUM'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[67] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'COMMENT', 3: 'STRLIT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[68] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[69] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[70] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[71] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[72] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[73] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[74] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[75] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[76] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: '__ANON_0',
   6: 'COMMA',
   7: 'LBRACE',
   8: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[77] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[78] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)',
  {1: 'COMMENT', 3: 'WS', 4: '__ANON_0'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[79] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[80] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[81] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[82] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[83] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[84] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[85] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS', 7: 'RPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[86] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[87] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[88] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COMMA',
   15: 'LBRACE',
   16: 'LESSTHAN',
   17: 'MINUS',
   18: 'MORETHAN',
   19: 'PERCENT',
   20: 'PLUS',
   21: 'RPAR',
   22: 'SEMICOLON',
   23: 'SLASH',
   24: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[89] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<__ANON_0>\\->)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: '__ANON_0',
   5: 'COMMA',
   6: 'LBRACE',
   7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[90] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[91] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[92] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[93] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[94] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[95] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[96] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[97] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[98] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[99] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[100] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[101] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[102] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[103] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[104] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[105] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'LBRACE', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[106] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[107] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[108] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: 'RPAR',
   9: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[109] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[110] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[111] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[112] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[113] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[114] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_0>\\->)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<COLON>:)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<LESSTHAN><)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_0',
   8: '__ANON_2',
   9: '__ANON_3',
   10: '__ANON_4',
   11: '__ANON_5',
   12: '__ANON_6',
   13: '__ANON_7',
   14: 'COLON',
   15: 'COMMA',
   16: 'LBRACE',
   17: 'LESSTHAN',
   18: 'LPAR',
   19: 'MINUS',
   20: 'MORETHAN',
   21: 'PERCENT',
   22: 'PLUS',
   23: 'RPAR',
   24: 'SEMICOLON',
   25: 'SLASH',
   26: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[115] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[116] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[117] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[118] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<LPAR>\\()',
  {1: 'COMMENT', 3: 'WS', 4: 'LPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[119] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[120] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[121] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[122] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[123] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[124] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[125] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[126] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[127] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[128] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RBRACE>\\})',
  {1: 'COMMENT', 3: 'WS', 4: 'RBRACE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[129] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[130] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[131] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[132] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<LPAR>\\()',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS', 5: 'INT', 6: 'LPAR'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[133] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[134] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[135] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[136] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[137] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[138] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[139] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[140] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[141] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[142] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[143] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[144] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[145] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[146] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[147] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[148] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[149] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[150] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[151] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<THEN>then)',
  {1: 'COMMENT', 3: 'WS', 4: 'THEN'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[152] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[153] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[154] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[155] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[156] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[157] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[158] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[159] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[160] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[161] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[162] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<PREDICATE>predicate)|(?P<FUNC>func)|(?P<INV>inv)',
  {1: 'COMMENT', 3: 'WS', 4: 'PREDICATE', 5: 'FUNC', 6: 'INV'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[163] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[164] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[165] = (lexer_regexps)
MRES = (
[('(?P<SNUMBER>(?:(?:\\+|\\-))?(?:(?:(?:[0-9])+(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+|(?:(?:[0-9])+\\.(?:(?:[0-9])+)?|\\.(?:[0-9])+)(?:(?:e|E)(?:(?:\\+|\\-))?(?:[0-9])+)?)|(?:[0-9])+))|(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<STRLIT>"(?:(?:\\\\"|[^"]))*")|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'SNUMBER', 2: 'NAME', 3: 'COMMENT', 5: 'STRLIT', 6: 'WS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[166] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)',
  {1: 'NAME', 2: 'COMMENT', 4: 'WS'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[167] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[168] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: 'RPAR',
   10: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[169] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE', 5: 'THEN', 6: 'RPAR', 7: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[170] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[171] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MORETHAN>>)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MORETHAN',
   15: 'RPAR',
   16: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[172] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[173] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[174] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[175] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[176] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PLUS',
   17: 'RPAR',
   18: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[177] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)|(?P<RBRACE>\\})',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS',
   9: 'RBRACE'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)|(?P<IF>if$)',
           {1: 'FALSE', 2: 'TRUE', 3: 'IF'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[178] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<RPAR>\\))',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'RPAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[179] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<COMMA>,)|(?P<LBRACE>\\{)|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'COMMA', 5: 'LBRACE', 6: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[180] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)|(?P<THEN>then)|(?P<__ANON_1>==>)|(?P<__ANON_2>\\|\\|)|(?P<__ANON_3>\\&\\&)|(?P<__ANON_4>==)|(?P<__ANON_5>!=)|(?P<__ANON_6><=)|(?P<__ANON_7>>=)|(?P<LESSTHAN><)|(?P<MINUS>\\-)|(?P<MORETHAN>>)|(?P<PERCENT>%)|(?P<PLUS>\\+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)|(?P<SLASH>/)|(?P<STAR>\\*)',
  {1: 'COMMENT',
   3: 'WS',
   4: 'ELSE',
   5: 'THEN',
   6: '__ANON_1',
   7: '__ANON_2',
   8: '__ANON_3',
   9: '__ANON_4',
   10: '__ANON_5',
   11: '__ANON_6',
   12: '__ANON_7',
   13: 'LESSTHAN',
   14: 'MINUS',
   15: 'MORETHAN',
   16: 'PERCENT',
   17: 'PLUS',
   18: 'RPAR',
   19: 'SEMICOLON',
   20: 'SLASH',
   21: 'STAR'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[181] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<ELSE>else)',
  {1: 'COMMENT', 3: 'WS', 4: 'ELSE'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[182] = (lexer_regexps)
MRES = (
[('(?P<NAME>(?:_|(?:[A-Z]|[a-z]))(?:(?:(?:_|(?:[A-Z]|[a-z]))|[0-9]))*)|(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<INT>(?:[0-9])+)|(?P<BANG>!)|(?P<LPAR>\\()|(?P<MINUS>\\-)',
  {1: 'NAME',
   2: 'COMMENT',
   4: 'WS',
   5: 'INT',
   6: 'BANG',
   7: 'LPAR',
   8: 'MINUS'})]
)
LEXER_CALLBACK = (
{'NAME': [('(?P<FALSE>false$)|(?P<TRUE>true$)', {1: 'FALSE', 2: 'TRUE'})]}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[183] = (lexer_regexps)
MRES = (
[('(?P<COMMENT>(?:\\#.*\\\n'
  '|\\(\\*(.|\n'
  ')+\\*\\)))|(?P<WS>(?:[ \t\x0c'
  '\r\n'
  '])+)|(?P<RPAR>\\))|(?P<SEMICOLON>;)',
  {1: 'COMMENT', 3: 'WS', 4: 'RPAR', 5: 'SEMICOLON'})]
)
LEXER_CALLBACK = (
{}
)
lexer_regexps = LexerRegexps()
lexer_regexps.mres = [(re.compile(p), d) for p, d in MRES]
lexer_regexps.callback = {n: UnlessCallback([(re.compile(p), d) for p, d in mres])
                          for n, mres in LEXER_CALLBACK.items()}
LEXERS[184] = (lexer_regexps)
class ContextualLexer:
    def __init__(self):
        self.lexers = LEXERS
        self.set_parser_state(None)
    def set_parser_state(self, state):
        self.parser_state = state
    def lex(self, stream):
        newline_types = NEWLINE_TYPES
        ignore_types = IGNORE_TYPES
        lexers = LEXERS
        l = _Lex(lexers[self.parser_state], self.parser_state)
        for x in l.lex(stream, newline_types, ignore_types):
            yield x
            l.lexer = lexers[self.parser_state]
            l.state = self.parser_state
CON_LEXER = ContextualLexer()
def lex(stream):
    return CON_LEXER.lex(stream)
RULES = {
  0: Rule(NonTerminal('start'), [NonTerminal('type_decls'), NonTerminal('program_decl'), NonTerminal('func_decls'), NonTerminal('inv_decls'), NonTerminal('global_preds')], None, RuleOptions(False, False, None)),
  1: Rule(NonTerminal('type_decls'), [NonTerminal('__anon_star_0')], None, RuleOptions(False, False, None)),
  2: Rule(NonTerminal('type_decls'), [], None, RuleOptions(False, False, None)),
  3: Rule(NonTerminal('type_decl'), [NonTerminal('enum_decl')], None, RuleOptions(False, True, None)),
  4: Rule(NonTerminal('type_decl'), [NonTerminal('value_decl')], None, RuleOptions(False, True, None)),
  5: Rule(NonTerminal('type_decl'), [NonTerminal('enum_set_decl')], None, RuleOptions(False, True, None)),
  6: Rule(NonTerminal('enum_decl'), [Terminal('ENUM', True), NonTerminal('type_name'), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  7: Rule(NonTerminal('enum_set_decl'), [Terminal('ENUMSET', True), NonTerminal('type_name'), Terminal('LSQB', True), Terminal('INT', False), Terminal('RSQB', True), NonTerminal('enum_body')], None, RuleOptions(False, False, None)),
  8: Rule(NonTerminal('enum_body'), [Terminal('LBRACE', True), NonTerminal('enum_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  9: Rule(NonTerminal('enum_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  10: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item'), NonTerminal('__anon_star_1')], None, RuleOptions(False, False, None)),
  11: Rule(NonTerminal('enum_items'), [NonTerminal('enum_item')], None, RuleOptions(False, False, None)),
  12: Rule(NonTerminal('value_decl'), [Terminal('VALUE', True), NonTerminal('type_name'), NonTerminal('value_body')], None, RuleOptions(False, False, None)),
  13: Rule(NonTerminal('value_body'), [Terminal('LBRACE', True), NonTerminal('value_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  14: Rule(NonTerminal('value_body'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  15: Rule(NonTerminal('value_items'), [NonTerminal('value_item')], None, RuleOptions(False, False, None)),
  16: Rule(NonTerminal('value_items'), [NonTerminal('value_item'), NonTerminal('__anon_star_2')], None, RuleOptions(False, False, None)),
  17: Rule(NonTerminal('value_item'), [NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('expr_type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  18: Rule(NonTerminal('program_decl'), [Terminal('PROGRAM', True), NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('type_names'), Terminal('RPAR', True), Terminal('__ANON_0', True), NonTerminal('type_name'), Terminal('SEMICOLON', True)], None, RuleOptions(False, False, None)),
  19: Rule(NonTerminal('func_decls'), [], None, RuleOptions(False, False, None)),
  20: Rule(NonTerminal('func_decls'), [NonTerminal('__anon_star_3')], None, RuleOptions(False, False, None)),
  21: Rule(NonTerminal('inv_decls'), [], None, RuleOptions(False, False, None)),
  22: Rule(NonTerminal('inv_decls'), [NonTerminal('__anon_star_4')], None, RuleOptions(False, False, None)),
  23: Rule(NonTerminal('func_decl'), [Terminal('FUNC', True), NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('func_body'), NonTerminal('func_constraints')], None, RuleOptions(False, False, None)),
  24: Rule(NonTerminal('inv_decl'), [Terminal('INV', True), NonTerminal('func_name'), Terminal('COLON', True), NonTerminal('func_body'), NonTerminal('func_constraints')], None, RuleOptions(False, False, None)),
  25: Rule(NonTerminal('func_body'), [NonTerminal('func_lhs'), Terminal('__ANON_0', True), NonTerminal('func_rhss')], None, RuleOptions(False, False, None)),
  26: Rule(NonTerminal('func_lhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  27: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs'), NonTerminal('__anon_star_5')], None, RuleOptions(False, False, None)),
  28: Rule(NonTerminal('func_rhss'), [NonTerminal('func_rhs')], None, RuleOptions(False, False, None)),
  29: Rule(NonTerminal('func_rhs'), [NonTerminal('opt_arg')], None, RuleOptions(False, True, None)),
  30: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name'), NonTerminal('var_name')], None, RuleOptions(False, False, None)),
  31: Rule(NonTerminal('opt_arg'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  32: Rule(NonTerminal('func_constraints'), [Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  33: Rule(NonTerminal('func_constraints'), [Terminal('LBRACE', True), NonTerminal('func_constraint_items'), Terminal('RBRACE', True)], None, RuleOptions(False, True, None)),
  34: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item')], None, RuleOptions(False, False, None)),
  35: Rule(NonTerminal('func_constraint_items'), [NonTerminal('func_constraint_item'), NonTerminal('__anon_star_6')], None, RuleOptions(False, False, None)),
  36: Rule(NonTerminal('func_constraint_item'), [NonTerminal('expr'), Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  37: Rule(NonTerminal('expr'), [NonTerminal('cond_expr')], None, RuleOptions(False, True, None)),
  38: Rule(NonTerminal('expr'), [NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  39: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr'), Terminal('__ANON_1', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  40: Rule(NonTerminal('imply_expr'), [NonTerminal('or_expr')], None, RuleOptions(False, True, None)),
  41: Rule(NonTerminal('or_expr'), [NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  42: Rule(NonTerminal('or_expr'), [NonTerminal('or_expr'), Terminal('__ANON_2', True), NonTerminal('and_expr')], None, RuleOptions(False, True, None)),
  43: Rule(NonTerminal('and_expr'), [NonTerminal('and_expr'), Terminal('__ANON_3', True), NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  44: Rule(NonTerminal('and_expr'), [NonTerminal('cmp_expr')], None, RuleOptions(False, True, None)),
  45: Rule(NonTerminal('cmp_expr'), [NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  46: Rule(NonTerminal('cmp_expr'), [NonTerminal('cmp_expr'), NonTerminal('cmp_op'), NonTerminal('term_expr')], None, RuleOptions(False, True, None)),
  47: Rule(NonTerminal('cmp_op'), [Terminal('LESSTHAN', True)], 'expr_lt', RuleOptions(False, True, None)),
  48: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_5', True)], 'expr_ne', RuleOptions(False, True, None)),
  49: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_4', True)], 'expr_eq', RuleOptions(False, True, None)),
  50: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_6', True)], 'expr_le', RuleOptions(False, True, None)),
  51: Rule(NonTerminal('cmp_op'), [Terminal('__ANON_7', True)], 'expr_ge', RuleOptions(False, True, None)),
  52: Rule(NonTerminal('cmp_op'), [Terminal('MORETHAN', True)], 'expr_gt', RuleOptions(False, True, None)),
  53: Rule(NonTerminal('term_expr'), [NonTerminal('term_expr'), NonTerminal('term_op'), NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  54: Rule(NonTerminal('term_expr'), [NonTerminal('factor_expr')], None, RuleOptions(False, True, None)),
  55: Rule(NonTerminal('term_op'), [Terminal('PLUS', True)], 'expr_add', RuleOptions(False, True, None)),
  56: Rule(NonTerminal('term_op'), [Terminal('MINUS', True)], 'expr_sub', RuleOptions(False, True, None)),
  57: Rule(NonTerminal('factor_expr'), [NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  58: Rule(NonTerminal('factor_expr'), [NonTerminal('factor_expr'), NonTerminal('factor_op'), NonTerminal('unary_expr')], None, RuleOptions(False, True, None)),
  59: Rule(NonTerminal('factor_op'), [Terminal('PERCENT', True)], 'expr_mod', RuleOptions(False, True, None)),
  60: Rule(NonTerminal('factor_op'), [Terminal('STAR', True)], 'expr_mul', RuleOptions(False, True, None)),
  61: Rule(NonTerminal('factor_op'), [Terminal('SLASH', True)], 'expr_div', RuleOptions(False, True, None)),
  62: Rule(NonTerminal('unary_expr'), [NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  63: Rule(NonTerminal('unary_expr'), [NonTerminal('unary_op'), NonTerminal('atom_expr')], None, RuleOptions(False, True, None)),
  64: Rule(NonTerminal('unary_op'), [Terminal('MINUS', True)], 'expr_neg', RuleOptions(False, True, None)),
  65: Rule(NonTerminal('unary_op'), [Terminal('BANG', True)], 'expr_not', RuleOptions(False, True, None)),
  66: Rule(NonTerminal('atom_expr'), [NonTerminal('const_expr')], None, RuleOptions(False, True, None)),
  67: Rule(NonTerminal('atom_expr'), [NonTerminal('property_expr')], None, RuleOptions(False, True, None)),
  68: Rule(NonTerminal('atom_expr'), [NonTerminal('var_expr')], None, RuleOptions(False, True, None)),
  69: Rule(NonTerminal('atom_expr'), [Terminal('LPAR', True), NonTerminal('expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  70: Rule(NonTerminal('const_expr'), [Terminal('FALSE', True)], 'expr_false', RuleOptions(False, True, None)),
  71: Rule(NonTerminal('const_expr'), [Terminal('TRUE', True)], 'expr_true', RuleOptions(False, True, None)),
  72: Rule(NonTerminal('const_expr'), [Terminal('INT', False)], 'expr_intlit', RuleOptions(False, True, None)),
  73: Rule(NonTerminal('var_expr'), [NonTerminal('var_name')], 'expr_var', RuleOptions(False, True, None)),
  74: Rule(NonTerminal('cond_expr'), [Terminal('IF', True), NonTerminal('imply_expr'), Terminal('THEN', True), NonTerminal('imply_expr'), Terminal('ELSE', True), NonTerminal('imply_expr')], None, RuleOptions(False, True, None)),
  75: Rule(NonTerminal('property_expr'), [NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('var_expr'), Terminal('RPAR', True)], None, RuleOptions(False, True, None)),
  76: Rule(NonTerminal('type_names'), [NonTerminal('type_name'), NonTerminal('__anon_star_7')], None, RuleOptions(False, False, None)),
  77: Rule(NonTerminal('type_names'), [], None, RuleOptions(False, False, None)),
  78: Rule(NonTerminal('type_names'), [NonTerminal('type_name')], None, RuleOptions(False, False, None)),
  79: Rule(NonTerminal('global_preds'), [], None, RuleOptions(False, False, None)),
  80: Rule(NonTerminal('global_preds'), [NonTerminal('__anon_star_8')], None, RuleOptions(False, False, None)),
  81: Rule(NonTerminal('global_pred'), [Terminal('PREDICATE', True), NonTerminal('pred_body'), Terminal('SEMICOLON', True)], None, RuleOptions(False, True, None)),
  82: Rule(NonTerminal('pred_body'), [NonTerminal('func_name'), Terminal('LPAR', True), NonTerminal('pred_args'), Terminal('RPAR', True)], None, RuleOptions(False, False, None)),
  83: Rule(NonTerminal('pred_args'), [NonTerminal('pred_arg'), NonTerminal('__anon_star_9')], None, RuleOptions(False, False, None)),
  84: Rule(NonTerminal('pred_args'), [NonTerminal('pred_arg')], None, RuleOptions(False, False, None)),
  85: Rule(NonTerminal('pred_args'), [], None, RuleOptions(False, False, None)),
  86: Rule(NonTerminal('pred_arg'), [Terminal('STRLIT', False)], 'pred_str', RuleOptions(False, False, None)),
  87: Rule(NonTerminal('pred_arg'), [Terminal('SNUMBER', False)], 'pred_num', RuleOptions(False, False, None)),
  88: Rule(NonTerminal('pred_arg'), [NonTerminal('var_name')], 'pred_var', RuleOptions(False, False, None)),
  89: Rule(NonTerminal('pred_arg'), [Terminal('FALSE', True)], 'pred_false', RuleOptions(False, False, None)),
  90: Rule(NonTerminal('pred_arg'), [Terminal('TRUE', True)], 'pred_true', RuleOptions(False, False, None)),
  91: Rule(NonTerminal('enum_item'), [Terminal('STRLIT', False)], None, RuleOptions(False, True, None)),
  92: Rule(NonTerminal('expr_type_name'), [Terminal('__ANON_8', True)], 'expr_int', RuleOptions(False, True, None)),
  93: Rule(NonTerminal('expr_type_name'), [Terminal('BOOL', True)], 'expr_bool', RuleOptions(False, True, None)),
  94: Rule(NonTerminal('type_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  95: Rule(NonTerminal('var_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  96: Rule(NonTerminal('func_name'), [Terminal('NAME', False)], None, RuleOptions(False, True, None)),
  97: Rule(NonTerminal('__anon_star_0'), [NonTerminal('__anon_star_0'), NonTerminal('type_decl')], None, None),
  98: Rule(NonTerminal('__anon_star_0'), [NonTerminal('type_decl')], None, None),
  99: Rule(NonTerminal('__anon_star_1'), [NonTerminal('__anon_star_1'), Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  100: Rule(NonTerminal('__anon_star_1'), [Terminal('COMMA', True), NonTerminal('enum_item')], None, None),
  101: Rule(NonTerminal('__anon_star_2'), [NonTerminal('value_item')], None, None),
  102: Rule(NonTerminal('__anon_star_2'), [NonTerminal('__anon_star_2'), NonTerminal('value_item')], None, None),
  103: Rule(NonTerminal('__anon_star_3'), [NonTerminal('func_decl')], None, None),
  104: Rule(NonTerminal('__anon_star_3'), [NonTerminal('__anon_star_3'), NonTerminal('func_decl')], None, None),
  105: Rule(NonTerminal('__anon_star_4'), [NonTerminal('__anon_star_4'), NonTerminal('inv_decl')], None, None),
  106: Rule(NonTerminal('__anon_star_4'), [NonTerminal('inv_decl')], None, None),
  107: Rule(NonTerminal('__anon_star_5'), [Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  108: Rule(NonTerminal('__anon_star_5'), [NonTerminal('__anon_star_5'), Terminal('COMMA', True), NonTerminal('func_rhs')], None, None),
  109: Rule(NonTerminal('__anon_star_6'), [NonTerminal('func_constraint_item')], None, None),
  110: Rule(NonTerminal('__anon_star_6'), [NonTerminal('__anon_star_6'), NonTerminal('func_constraint_item')], None, None),
  111: Rule(NonTerminal('__anon_star_7'), [Terminal('COMMA', True), NonTerminal('type_name')], None, None),
  112: Rule(NonTerminal('__anon_star_7'), [NonTerminal('__anon_star_7'), Terminal('COMMA', True), NonTerminal('type_name')], None, None),
  113: Rule(NonTerminal('__anon_star_8'), [NonTerminal('__anon_star_8'), NonTerminal('global_pred')], None, None),
  114: Rule(NonTerminal('__anon_star_8'), [NonTerminal('global_pred')], None, None),
  115: Rule(NonTerminal('__anon_star_9'), [Terminal('COMMA', True), NonTerminal('pred_arg')], None, None),
  116: Rule(NonTerminal('__anon_star_9'), [NonTerminal('__anon_star_9'), Terminal('COMMA', True), NonTerminal('pred_arg')], None, None),
}
parse_tree_builder = ParseTreeBuilder(RULES.values(), Tree)
class ParseTable: pass
parse_table = ParseTable()
STATES = {
  0: {0: (1, 2), 1: (0, 1), 2: (0, 2), 3: (0, 3), 4: (0, 4), 5: (0, 5), 6: (0, 6), 7: (0, 7), 8: (0, 8), 9: (0, 9), 10: (0, 10)},
  1: {11: (0, 11), 12: (0, 12)},
  2: {11: (0, 11), 12: (0, 13)},
  3: {2: (1, 3), 6: (1, 3), 1: (1, 3), 0: (1, 3)},
  4: {0: (1, 1), 1: (0, 1), 2: (0, 2), 3: (0, 3), 5: (0, 5), 9: (0, 14), 6: (0, 6), 10: (0, 10)},
  5: {2: (1, 4), 6: (1, 4), 1: (1, 4), 0: (1, 4)},
  6: {11: (0, 11), 12: (0, 15)},
  7: {13: (0, 16)},
  8: {0: (0, 17), 14: (0, 18)},
  9: {6: (1, 98), 1: (1, 98), 2: (1, 98), 0: (1, 98)},
  10: {2: (1, 5), 6: (1, 5), 1: (1, 5), 0: (1, 5)},
  11: {15: (1, 94), 16: (1, 94), 17: (1, 94), 18: (1, 94), 19: (1, 94), 11: (1, 94), 20: (1, 94)},
  12: {16: (0, 19), 15: (0, 20), 21: (0, 21)},
  13: {18: (0, 22)},
  14: {6: (1, 97), 1: (1, 97), 2: (1, 97), 0: (1, 97)},
  15: {22: (0, 23), 16: (0, 24), 15: (0, 25)},
  16: {},
  17: {11: (0, 26), 23: (0, 27)},
  18: {24: (1, 19), 13: (1, 19), 25: (1, 19), 26: (0, 28), 27: (0, 29), 28: (0, 30), 29: (0, 31)},
  19: {23: (0, 32), 30: (0, 33), 31: (0, 34), 11: (0, 26)},
  20: {2: (1, 14), 6: (1, 14), 1: (1, 14), 0: (1, 14)},
  21: {2: (1, 12), 6: (1, 12), 1: (1, 12), 0: (1, 12)},
  22: {32: (0, 35)},
  23: {2: (1, 6), 6: (1, 6), 1: (1, 6), 0: (1, 6)},
  24: {33: (0, 36), 34: (0, 37), 35: (0, 38)},
  25: {2: (1, 9), 6: (1, 9), 1: (1, 9), 0: (1, 9)},
  26: {36: (1, 96), 37: (1, 96)},
  27: {37: (0, 39)},
  28: {13: (1, 21), 25: (1, 21), 38: (0, 40), 39: (0, 41), 40: (0, 42), 24: (0, 43)},
  29: {28: (1, 103), 24: (1, 103), 13: (1, 103), 25: (1, 103)},
  30: {23: (0, 44), 11: (0, 26)},
  31: {24: (1, 20), 13: (1, 20), 25: (1, 20), 27: (0, 45), 28: (0, 30)},
  32: {36: (0, 46)},
  33: {41: (1, 15), 30: (0, 47), 42: (0, 48), 23: (0, 32), 11: (0, 26)},
  34: {41: (0, 49)},
  35: {43: (0, 50)},
  36: {41: (1, 11), 44: (0, 51), 17: (0, 52)},
  37: {41: (1, 91), 17: (1, 91)},
  38: {41: (0, 53)},
  39: {20: (1, 77), 11: (0, 11), 12: (0, 54), 45: (0, 55)},
  40: {13: (1, 22), 25: (1, 22), 39: (0, 56), 24: (0, 43)},
  41: {24: (1, 106), 13: (1, 106), 25: (1, 106)},
  42: {13: (1, 79), 46: (0, 57), 47: (0, 58), 48: (0, 59), 25: (0, 60)},
  43: {11: (0, 26), 23: (0, 61)},
  44: {36: (0, 62)},
  45: {28: (1, 104), 24: (1, 104), 13: (1, 104), 25: (1, 104)},
  46: {49: (0, 63), 50: (0, 64), 51: (0, 65)},
  47: {41: (1, 101), 11: (1, 101)},
  48: {41: (1, 16), 23: (0, 32), 11: (0, 26), 30: (0, 66)},
  49: {2: (1, 13), 6: (1, 13), 1: (1, 13), 0: (1, 13)},
  50: {22: (0, 67), 16: (0, 24), 15: (0, 25)},
  51: {41: (1, 10), 17: (0, 68)},
  52: {34: (0, 37), 33: (0, 69)},
  53: {2: (1, 8), 6: (1, 8), 1: (1, 8), 0: (1, 8)},
  54: {20: (1, 78), 52: (0, 70), 17: (0, 71)},
  55: {20: (0, 72)},
  56: {24: (1, 105), 13: (1, 105), 25: (1, 105)},
  57: {13: (1, 80), 47: (0, 73), 25: (0, 60)},
  58: {13: (1, 114), 25: (1, 114)},
  59: {13: (1, 0)},
  60: {23: (0, 74), 11: (0, 26), 53: (0, 75)},
  61: {36: (0, 76)},
  62: {12: (0, 77), 11: (0, 11), 54: (0, 78), 55: (0, 79), 56: (0, 80)},
  63: {15: (1, 93)},
  64: {15: (1, 92)},
  65: {15: (0, 81)},
  66: {41: (1, 102), 11: (1, 102)},
  67: {2: (1, 7), 6: (1, 7), 1: (1, 7), 0: (1, 7)},
  68: {34: (0, 37), 33: (0, 82)},
  69: {41: (1, 100), 17: (1, 100)},
  70: {20: (1, 76), 17: (0, 83)},
  71: {12: (0, 84), 11: (0, 11)},
  72: {19: (0, 85)},
  73: {13: (1, 113), 25: (1, 113)},
  74: {37: (0, 86)},
  75: {15: (0, 87)},
  76: {12: (0, 77), 11: (0, 11), 54: (0, 78), 55: (0, 79), 56: (0, 88)},
  77: {15: (1, 31), 19: (1, 31), 16: (1, 31), 17: (1, 31), 11: (0, 89), 57: (0, 90)},
  78: {19: (0, 91)},
  79: {19: (1, 26)},
  80: {58: (0, 92), 15: (0, 93), 16: (0, 94)},
  81: {41: (1, 17), 11: (1, 17)},
  82: {41: (1, 99), 17: (1, 99)},
  83: {12: (0, 95), 11: (0, 11)},
  84: {17: (1, 111), 20: (1, 111)},
  85: {11: (0, 11), 12: (0, 96)},
  86: {20: (1, 85), 59: (0, 97), 60: (0, 98), 61: (0, 99), 34: (0, 100), 62: (0, 101), 57: (0, 102), 63: (0, 103), 11: (0, 89)},
  87: {13: (1, 81), 25: (1, 81)},
  88: {58: (0, 104), 15: (0, 93), 16: (0, 94)},
  89: {64: (1, 95), 65: (1, 95), 66: (1, 95), 67: (1, 95), 68: (1, 95), 69: (1, 95), 70: (1, 95), 20: (1, 95), 71: (1, 95), 72: (1, 95), 19: (1, 95), 15: (1, 95), 16: (1, 95), 17: (1, 95), 73: (1, 95), 74: (1, 95), 75: (1, 95), 76: (1, 95), 77: (1, 95), 78: (1, 95), 79: (1, 95)},
  90: {15: (1, 30), 19: (1, 30), 16: (1, 30), 17: (1, 30)},
  91: {55: (0, 105), 12: (0, 77), 80: (0, 106), 11: (0, 11), 81: (0, 107)},
  92: {28: (1, 23), 24: (1, 23), 13: (1, 23), 25: (1, 23)},
  93: {28: (1, 32), 24: (1, 32), 13: (1, 32), 25: (1, 32)},
  94: {57: (0, 108), 82: (0, 109), 83: (0, 110), 84: (0, 111), 85: (0, 112), 86: (0, 113), 87: (0, 114), 11: (0, 115), 63: (0, 116), 88: (0, 117), 89: (0, 118), 23: (0, 119), 90: (0, 120), 91: (0, 121), 77: (0, 122), 92: (0, 123), 37: (0, 124), 93: (0, 125), 59: (0, 126), 94: (0, 127), 32: (0, 128), 95: (0, 129), 96: (0, 130), 97: (0, 131), 98: (0, 132), 99: (0, 133)},
  95: {17: (1, 112), 20: (1, 112)},
  96: {15: (0, 134)},
  97: {20: (1, 89), 17: (1, 89)},
  98: {20: (1, 87), 17: (1, 87)},
  99: {20: (0, 135)},
  100: {20: (1, 86), 17: (1, 86)},
  101: {20: (1, 84), 17: (0, 136), 100: (0, 137)},
  102: {20: (1, 88), 17: (1, 88)},
  103: {20: (1, 90), 17: (1, 90)},
  104: {24: (1, 24), 13: (1, 24), 25: (1, 24)},
  105: {15: (1, 29), 16: (1, 29), 17: (1, 29)},
  106: {15: (1, 25), 16: (1, 25)},
  107: {15: (1, 28), 16: (1, 28), 101: (0, 138), 17: (0, 139)},
  108: {64: (1, 73), 65: (1, 73), 66: (1, 73), 67: (1, 73), 68: (1, 73), 69: (1, 73), 70: (1, 73), 20: (1, 73), 71: (1, 73), 72: (1, 73), 15: (1, 73), 73: (1, 73), 74: (1, 73), 75: (1, 73), 76: (1, 73), 77: (1, 73), 78: (1, 73), 79: (1, 73)},
  109: {15: (1, 40), 68: (1, 40), 73: (1, 40), 20: (1, 40), 79: (0, 140), 72: (0, 141)},
  110: {15: (1, 54), 64: (1, 54), 65: (1, 54), 66: (1, 54), 68: (1, 54), 69: (1, 54), 73: (1, 54), 20: (1, 54), 75: (1, 54), 71: (1, 54), 76: (1, 54), 77: (1, 54), 72: (1, 54), 78: (1, 54), 79: (1, 54), 102: (0, 142), 67: (0, 143), 70: (0, 144), 74: (0, 145)},
  111: {15: (1, 37), 20: (1, 37)},
  112: {15: (1, 41), 72: (1, 41), 68: (1, 41), 73: (1, 41), 79: (1, 41), 20: (1, 41), 66: (0, 146)},
  113: {15: (1, 68), 64: (1, 68), 65: (1, 68), 66: (1, 68), 67: (1, 68), 68: (1, 68), 69: (1, 68), 73: (1, 68), 74: (1, 68), 70: (1, 68), 20: (1, 68), 75: (1, 68), 71: (1, 68), 76: (1, 68), 77: (1, 68), 72: (1, 68), 78: (1, 68), 79: (1, 68)},
  114: {15: (1, 66), 64: (1, 66), 65: (1, 66), 66: (1, 66), 67: (1, 66), 68: (1, 66), 69: (1, 66), 73: (1, 66), 74: (1, 66), 70: (1, 66), 20: (1, 66), 75: (1, 66), 71: (1, 66), 76: (1, 66), 77: (1, 66), 72: (1, 66), 78: (1, 66), 79: (1, 66)},
  115: {36: (1, 96), 37: (1, 96), 64: (1, 95), 65: (1, 95), 66: (1, 95), 67: (1, 95), 68: (1, 95), 69: (1, 95), 70: (1, 95), 20: (1, 95), 71: (1, 95), 72: (1, 95), 19: (1, 95), 15: (1, 95), 16: (1, 95), 17: (1, 95), 73: (1, 95), 74: (1, 95), 75: (1, 95), 76: (1, 95), 77: (1, 95), 78: (1, 95), 79: (1, 95)},
  116: {64: (1, 71), 65: (1, 71), 66: (1, 71), 67: (1, 71), 68: (1, 71), 69: (1, 71), 70: (1, 71), 20: (1, 71), 71: (1, 71), 72: (1, 71), 15: (1, 71), 73: (1, 71), 74: (1, 71), 75: (1, 71), 76: (1, 71), 77: (1, 71), 78: (1, 71), 79: (1, 71)},
  117: {15: (0, 147)},
  118: {15: (1, 45), 64: (1, 45), 65: (1, 45), 66: (1, 45), 68: (1, 45), 69: (1, 45), 73: (1, 45), 20: (1, 45), 75: (1, 45), 76: (1, 45), 72: (1, 45), 78: (1, 45), 79: (1, 45), 103: (0, 148), 71: (0, 149), 77: (0, 150)},
  119: {37: (0, 151)},
  120: {15: (1, 62), 64: (1, 62), 65: (1, 62), 66: (1, 62), 67: (1, 62), 68: (1, 62), 69: (1, 62), 73: (1, 62), 70: (1, 62), 74: (1, 62), 20: (1, 62), 75: (1, 62), 71: (1, 62), 76: (1, 62), 77: (1, 62), 72: (1, 62), 78: (1, 62), 79: (1, 62)},
  121: {82: (0, 109), 57: (0, 108), 37: (0, 124), 83: (0, 110), 93: (0, 125), 11: (0, 115), 59: (0, 126), 92: (0, 152), 85: (0, 112), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  122: {32: (1, 64), 63: (1, 64), 11: (1, 64), 37: (1, 64), 59: (1, 64)},
  123: {15: (1, 38), 20: (1, 38)},
  124: {92: (0, 123), 82: (0, 109), 57: (0, 108), 37: (0, 124), 83: (0, 110), 93: (0, 125), 11: (0, 115), 59: (0, 126), 84: (0, 111), 85: (0, 112), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 88: (0, 153), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 91: (0, 121), 77: (0, 122)},
  125: {15: (1, 44), 66: (1, 44), 72: (1, 44), 68: (1, 44), 73: (1, 44), 79: (1, 44), 20: (1, 44), 64: (0, 154), 69: (0, 155), 76: (0, 156), 78: (0, 157), 104: (0, 158), 65: (0, 159), 75: (0, 160)},
  126: {64: (1, 70), 65: (1, 70), 66: (1, 70), 67: (1, 70), 68: (1, 70), 69: (1, 70), 70: (1, 70), 20: (1, 70), 71: (1, 70), 72: (1, 70), 15: (1, 70), 73: (1, 70), 74: (1, 70), 75: (1, 70), 76: (1, 70), 77: (1, 70), 78: (1, 70), 79: (1, 70)},
  127: {41: (1, 34), 94: (0, 161), 57: (0, 108), 82: (0, 109), 83: (0, 110), 84: (0, 111), 85: (0, 112), 86: (0, 113), 87: (0, 114), 11: (0, 115), 63: (0, 116), 88: (0, 117), 89: (0, 118), 23: (0, 119), 90: (0, 120), 105: (0, 162), 91: (0, 121), 77: (0, 122), 92: (0, 123), 37: (0, 124), 93: (0, 125), 59: (0, 126), 32: (0, 128), 96: (0, 130), 97: (0, 131), 98: (0, 132), 99: (0, 133)},
  128: {64: (1, 72), 65: (1, 72), 66: (1, 72), 67: (1, 72), 68: (1, 72), 69: (1, 72), 70: (1, 72), 20: (1, 72), 71: (1, 72), 72: (1, 72), 15: (1, 72), 73: (1, 72), 74: (1, 72), 75: (1, 72), 76: (1, 72), 77: (1, 72), 78: (1, 72), 79: (1, 72)},
  129: {41: (0, 163)},
  130: {90: (0, 164), 63: (0, 116), 97: (0, 131), 57: (0, 108), 37: (0, 124), 11: (0, 115), 23: (0, 119), 59: (0, 126), 32: (0, 128), 86: (0, 113), 87: (0, 114)},
  131: {15: (1, 67), 64: (1, 67), 65: (1, 67), 66: (1, 67), 67: (1, 67), 68: (1, 67), 69: (1, 67), 73: (1, 67), 74: (1, 67), 70: (1, 67), 20: (1, 67), 75: (1, 67), 71: (1, 67), 76: (1, 67), 77: (1, 67), 72: (1, 67), 78: (1, 67), 79: (1, 67)},
  132: {15: (1, 57), 64: (1, 57), 65: (1, 57), 66: (1, 57), 67: (1, 57), 68: (1, 57), 69: (1, 57), 73: (1, 57), 70: (1, 57), 74: (1, 57), 20: (1, 57), 75: (1, 57), 71: (1, 57), 76: (1, 57), 77: (1, 57), 72: (1, 57), 78: (1, 57), 79: (1, 57)},
  133: {32: (1, 65), 63: (1, 65), 11: (1, 65), 37: (1, 65), 59: (1, 65)},
  134: {28: (1, 18), 24: (1, 18), 13: (1, 18), 25: (1, 18)},
  135: {15: (1, 82)},
  136: {59: (0, 97), 63: (0, 103), 60: (0, 98), 34: (0, 100), 11: (0, 89), 62: (0, 165), 57: (0, 102)},
  137: {20: (1, 83), 17: (0, 166)},
  138: {15: (1, 27), 16: (1, 27), 17: (0, 167)},
  139: {55: (0, 105), 12: (0, 77), 81: (0, 168), 11: (0, 11)},
  140: {57: (0, 108), 37: (0, 124), 85: (0, 169), 83: (0, 110), 93: (0, 125), 11: (0, 115), 59: (0, 126), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  141: {92: (0, 170), 82: (0, 109), 57: (0, 108), 37: (0, 124), 83: (0, 110), 93: (0, 125), 11: (0, 115), 59: (0, 126), 85: (0, 112), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  142: {98: (0, 171), 57: (0, 108), 37: (0, 124), 11: (0, 115), 59: (0, 126), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 23: (0, 119), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  143: {32: (1, 59), 99: (1, 59), 77: (1, 59), 63: (1, 59), 11: (1, 59), 37: (1, 59), 59: (1, 59)},
  144: {32: (1, 60), 99: (1, 60), 77: (1, 60), 63: (1, 60), 11: (1, 60), 37: (1, 60), 59: (1, 60)},
  145: {32: (1, 61), 99: (1, 61), 77: (1, 61), 63: (1, 61), 11: (1, 61), 37: (1, 61), 59: (1, 61)},
  146: {57: (0, 108), 37: (0, 124), 83: (0, 110), 11: (0, 115), 59: (0, 126), 32: (0, 128), 86: (0, 113), 93: (0, 172), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  147: {32: (1, 36), 99: (1, 36), 77: (1, 36), 41: (1, 36), 63: (1, 36), 91: (1, 36), 11: (1, 36), 37: (1, 36), 59: (1, 36)},
  148: {57: (0, 108), 37: (0, 124), 83: (0, 173), 11: (0, 115), 59: (0, 126), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  149: {32: (1, 55), 99: (1, 55), 77: (1, 55), 63: (1, 55), 11: (1, 55), 37: (1, 55), 59: (1, 55)},
  150: {32: (1, 56), 99: (1, 56), 77: (1, 56), 63: (1, 56), 11: (1, 56), 37: (1, 56), 59: (1, 56)},
  151: {11: (0, 89), 86: (0, 174), 57: (0, 108)},
  152: {68: (0, 175)},
  153: {20: (0, 176)},
  154: {32: (1, 47), 99: (1, 47), 77: (1, 47), 63: (1, 47), 11: (1, 47), 37: (1, 47), 59: (1, 47)},
  155: {32: (1, 51), 99: (1, 51), 77: (1, 51), 63: (1, 51), 11: (1, 51), 37: (1, 51), 59: (1, 51)},
  156: {32: (1, 49), 99: (1, 49), 77: (1, 49), 63: (1, 49), 11: (1, 49), 37: (1, 49), 59: (1, 49)},
  157: {32: (1, 48), 99: (1, 48), 77: (1, 48), 63: (1, 48), 11: (1, 48), 37: (1, 48), 59: (1, 48)},
  158: {57: (0, 108), 37: (0, 124), 83: (0, 110), 11: (0, 115), 59: (0, 126), 89: (0, 177), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  159: {32: (1, 50), 99: (1, 50), 77: (1, 50), 63: (1, 50), 11: (1, 50), 37: (1, 50), 59: (1, 50)},
  160: {32: (1, 52), 99: (1, 52), 77: (1, 52), 63: (1, 52), 11: (1, 52), 37: (1, 52), 59: (1, 52)},
  161: {32: (1, 109), 99: (1, 109), 77: (1, 109), 41: (1, 109), 63: (1, 109), 91: (1, 109), 11: (1, 109), 37: (1, 109), 59: (1, 109)},
  162: {41: (1, 35), 57: (0, 108), 82: (0, 109), 83: (0, 110), 84: (0, 111), 85: (0, 112), 86: (0, 113), 87: (0, 114), 11: (0, 115), 63: (0, 116), 88: (0, 117), 89: (0, 118), 23: (0, 119), 90: (0, 120), 91: (0, 121), 77: (0, 122), 92: (0, 123), 37: (0, 124), 93: (0, 125), 59: (0, 126), 94: (0, 178), 32: (0, 128), 96: (0, 130), 97: (0, 131), 98: (0, 132), 99: (0, 133)},
  163: {28: (1, 33), 24: (1, 33), 13: (1, 33), 25: (1, 33)},
  164: {15: (1, 63), 64: (1, 63), 65: (1, 63), 66: (1, 63), 67: (1, 63), 68: (1, 63), 69: (1, 63), 73: (1, 63), 70: (1, 63), 74: (1, 63), 20: (1, 63), 75: (1, 63), 71: (1, 63), 76: (1, 63), 77: (1, 63), 72: (1, 63), 78: (1, 63), 79: (1, 63)},
  165: {17: (1, 115), 20: (1, 115)},
  166: {59: (0, 97), 63: (0, 103), 60: (0, 98), 62: (0, 179), 34: (0, 100), 11: (0, 89), 57: (0, 102)},
  167: {55: (0, 105), 12: (0, 77), 11: (0, 11), 81: (0, 180)},
  168: {15: (1, 107), 16: (1, 107), 17: (1, 107)},
  169: {15: (1, 42), 72: (1, 42), 68: (1, 42), 73: (1, 42), 79: (1, 42), 20: (1, 42), 66: (0, 146)},
  170: {15: (1, 39), 68: (1, 39), 73: (1, 39), 20: (1, 39)},
  171: {15: (1, 58), 64: (1, 58), 65: (1, 58), 66: (1, 58), 67: (1, 58), 68: (1, 58), 69: (1, 58), 73: (1, 58), 70: (1, 58), 74: (1, 58), 20: (1, 58), 75: (1, 58), 71: (1, 58), 76: (1, 58), 77: (1, 58), 72: (1, 58), 78: (1, 58), 79: (1, 58)},
  172: {15: (1, 43), 66: (1, 43), 72: (1, 43), 68: (1, 43), 73: (1, 43), 79: (1, 43), 20: (1, 43), 64: (0, 154), 69: (0, 155), 76: (0, 156), 78: (0, 157), 104: (0, 158), 65: (0, 159), 75: (0, 160)},
  173: {15: (1, 53), 64: (1, 53), 65: (1, 53), 66: (1, 53), 68: (1, 53), 69: (1, 53), 73: (1, 53), 20: (1, 53), 75: (1, 53), 71: (1, 53), 76: (1, 53), 77: (1, 53), 72: (1, 53), 78: (1, 53), 79: (1, 53), 102: (0, 142), 67: (0, 143), 70: (0, 144), 74: (0, 145)},
  174: {20: (0, 181)},
  175: {82: (0, 109), 57: (0, 108), 37: (0, 124), 83: (0, 110), 93: (0, 125), 92: (0, 182), 11: (0, 115), 59: (0, 126), 85: (0, 112), 32: (0, 128), 86: (0, 113), 87: (0, 114), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  176: {15: (1, 69), 64: (1, 69), 65: (1, 69), 66: (1, 69), 67: (1, 69), 68: (1, 69), 69: (1, 69), 73: (1, 69), 74: (1, 69), 70: (1, 69), 20: (1, 69), 75: (1, 69), 71: (1, 69), 76: (1, 69), 77: (1, 69), 72: (1, 69), 78: (1, 69), 79: (1, 69)},
  177: {15: (1, 46), 64: (1, 46), 65: (1, 46), 66: (1, 46), 68: (1, 46), 69: (1, 46), 73: (1, 46), 20: (1, 46), 75: (1, 46), 76: (1, 46), 72: (1, 46), 78: (1, 46), 79: (1, 46), 103: (0, 148), 71: (0, 149), 77: (0, 150)},
  178: {32: (1, 110), 99: (1, 110), 77: (1, 110), 41: (1, 110), 63: (1, 110), 91: (1, 110), 11: (1, 110), 37: (1, 110), 59: (1, 110)},
  179: {17: (1, 116), 20: (1, 116)},
  180: {15: (1, 108), 16: (1, 108), 17: (1, 108)},
  181: {64: (1, 75), 65: (1, 75), 66: (1, 75), 67: (1, 75), 68: (1, 75), 69: (1, 75), 70: (1, 75), 20: (1, 75), 71: (1, 75), 72: (1, 75), 15: (1, 75), 73: (1, 75), 74: (1, 75), 75: (1, 75), 76: (1, 75), 77: (1, 75), 78: (1, 75), 79: (1, 75)},
  182: {73: (0, 183)},
  183: {82: (0, 109), 57: (0, 108), 37: (0, 124), 83: (0, 110), 93: (0, 125), 11: (0, 115), 59: (0, 126), 85: (0, 112), 32: (0, 128), 86: (0, 113), 87: (0, 114), 92: (0, 184), 96: (0, 130), 63: (0, 116), 97: (0, 131), 89: (0, 118), 23: (0, 119), 98: (0, 132), 99: (0, 133), 90: (0, 120), 77: (0, 122)},
  184: {15: (1, 74), 20: (1, 74)},
}
TOKEN_TYPES = (
{0: 'PROGRAM',
 1: 'VALUE',
 2: 'ENUMSET',
 3: 'enum_decl',
 4: '__anon_star_0',
 5: 'value_decl',
 6: 'ENUM',
 7: 'start',
 8: 'type_decls',
 9: 'type_decl',
 10: 'enum_set_decl',
 11: 'NAME',
 12: 'type_name',
 13: '$END',
 14: 'program_decl',
 15: 'SEMICOLON',
 16: 'LBRACE',
 17: 'COMMA',
 18: 'LSQB',
 19: '__ANON_0',
 20: 'RPAR',
 21: 'value_body',
 22: 'enum_body',
 23: 'func_name',
 24: 'INV',
 25: 'PREDICATE',
 26: 'func_decls',
 27: 'func_decl',
 28: 'FUNC',
 29: '__anon_star_3',
 30: 'value_item',
 31: 'value_items',
 32: 'INT',
 33: 'enum_item',
 34: 'STRLIT',
 35: 'enum_items',
 36: 'COLON',
 37: 'LPAR',
 38: '__anon_star_4',
 39: 'inv_decl',
 40: 'inv_decls',
 41: 'RBRACE',
 42: '__anon_star_2',
 43: 'RSQB',
 44: '__anon_star_1',
 45: 'type_names',
 46: '__anon_star_8',
 47: 'global_pred',
 48: 'global_preds',
 49: 'BOOL',
 50: '__ANON_8',
 51: 'expr_type_name',
 52: '__anon_star_7',
 53: 'pred_body',
 54: 'func_lhs',
 55: 'opt_arg',
 56: 'func_body',
 57: 'var_name',
 58: 'func_constraints',
 59: 'FALSE',
 60: 'SNUMBER',
 61: 'pred_args',
 62: 'pred_arg',
 63: 'TRUE',
 64: 'LESSTHAN',
 65: '__ANON_6',
 66: '__ANON_3',
 67: 'PERCENT',
 68: 'THEN',
 69: '__ANON_7',
 70: 'STAR',
 71: 'PLUS',
 72: '__ANON_1',
 73: 'ELSE',
 74: 'SLASH',
 75: 'MORETHAN',
 76: '__ANON_4',
 77: 'MINUS',
 78: '__ANON_5',
 79: '__ANON_2',
 80: 'func_rhss',
 81: 'func_rhs',
 82: 'or_expr',
 83: 'factor_expr',
 84: 'cond_expr',
 85: 'and_expr',
 86: 'var_expr',
 87: 'const_expr',
 88: 'expr',
 89: 'term_expr',
 90: 'atom_expr',
 91: 'IF',
 92: 'imply_expr',
 93: 'cmp_expr',
 94: 'func_constraint_item',
 95: 'func_constraint_items',
 96: 'unary_op',
 97: 'property_expr',
 98: 'unary_expr',
 99: 'BANG',
 100: '__anon_star_9',
 101: '__anon_star_5',
 102: 'factor_op',
 103: 'term_op',
 104: 'cmp_op',
 105: '__anon_star_6'}
)
parse_table.states = {s: {TOKEN_TYPES[t]: (a, RULES[x] if a is Reduce else x) for t, (a, x) in acts.items()}
                      for s, acts in STATES.items()}
parse_table.start_state = 0
parse_table.end_state = 16
class Lark_StandAlone:
  def __init__(self, transformer=None, postlex=None):
     callback = parse_tree_builder.create_callback(transformer=transformer)
     callbacks = {rule: getattr(callback, rule.alias or rule.origin, None) for rule in RULES.values()}
     self.parser = _Parser(parse_table, callbacks)
     self.postlex = postlex
  def parse(self, stream):
     tokens = lex(stream)
     sps = CON_LEXER.set_parser_state
     if self.postlex: tokens = self.postlex.process(tokens)
     return self.parser.parse(tokens, sps)
